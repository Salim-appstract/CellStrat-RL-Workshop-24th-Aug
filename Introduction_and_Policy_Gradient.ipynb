{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Introduction and Policy Gradient.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shubha-Manikarnike/CellStrat-RL-Workshop-24th-Aug/blob/master/Introduction_and_Policy_Gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6CnqYBgLVd2W"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oFDLmqp-Vd2b"
      },
      "source": [
        "First, let's make sure this notebook works well in both python 2 and 3, import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jniP8Ff_KEQP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "a07bf4cb-da38-47bf-cc53-95e5a1827f47"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXsVITeHKKEO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "114d955c-c510-4fc6-fff4-8aed10acab9c"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "os.chdir('/content/gdrive/My Drive/Colab Notebooks/RL Workshop')\n",
        "os.listdir()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Introduction and Policy Gradient.ipynb',\n",
              " 'Dynamic Programming.ipynb',\n",
              " 'Q- Learning.ipynb',\n",
              " 'MonteCarlo.ipynb',\n",
              " 'Untitled0.ipynb']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lPZx8455Vd2e",
        "colab": {}
      },
      "source": [
        "# To support both python 2 and python 3\n",
        "from __future__ import division, print_function, unicode_literals\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "def reset_graph(seed=42):\n",
        "    tf.reset_default_graph()\n",
        "    tf.set_random_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "# To plot pretty figures and animations\n",
        "#two lines added by CellStrat\n",
        "#!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "#!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "\n",
        "%matplotlib nbagg\n",
        "import matplotlib\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#this line added by CellStrat\n",
        "'''from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()'''\n",
        "#end CellStrat\n",
        "%matplotlib inline\n",
        "\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"rl\"\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True):\n",
        "    path = os.path.join(PROJECT_ROOT_DIR, CHAPTER_ID, fig_id + \".png\")\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format='png', dpi=300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wg7z9LOaVd2o"
      },
      "source": [
        "Note: there may be minor differences between the output of this notebook and the examples shown in the book. You can safely ignore these differences. They are mainly due to the fact that most of the environments provided by OpenAI gym have some randomness."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ih_7NndtVd2x",
        "colab": {}
      },
      "source": [
        "import gym"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1HMiFBnZVd4b"
      },
      "source": [
        "Let's create a little helper function to plot an environment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BKNdg43GVd4f",
        "colab": {}
      },
      "source": [
        "def plot_environment(env, figsize=(5,4)):\n",
        "    plt.close()  # or else nbagg sometimes plots in the previous cell\n",
        "    plt.figure(figsize=figsize)\n",
        "    img = env.render(mode=\"rgb_array\")\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovGLMBenITXm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_scene(num, frames, patch):\n",
        "    patch.set_data(frames[num])\n",
        "    return patch,\n",
        "\n",
        "def plot_animation(frames, repeat=False, interval=40):\n",
        "    plt.close()  # or else nbagg sometimes plots in the previous cell\n",
        "    fig = plt.figure()\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "    return animation.FuncAnimation(fig, update_scene, fargs=(frames, patch), frames=len(frames), repeat=repeat, interval=interval)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XEHne7-CVd4v"
      },
      "source": [
        "Let's see how to interact with an environment. Your agent will need to select an action from an \"action space\" (the set of possible actions). Let's see what this environment's action space looks like:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "X4ZRM0L8Vd7E"
      },
      "source": [
        "# A simple environment: the Cart-Pole"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m20VIQ1nVd7F"
      },
      "source": [
        "The Cart-Pole is a very simple environment composed of a cart that can move left or right, and pole placed vertically on top of it. The agent must move the cart left or right to keep the pole upright."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kneIxtJqVd7G",
        "colab": {}
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sHpDT89PVd7J",
        "colab": {}
      },
      "source": [
        "obs = env.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pB8e9mgxVd7N",
        "outputId": "c788f266-7500-4a12-9716-bcbd52786301",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "obs"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.00547143, -0.02782027,  0.00641956, -0.01624609])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "13NAgMwlVd7R"
      },
      "source": [
        "The observation is a 1D NumPy array composed of 4 floats: they represent the cart's horizontal position, its velocity, the angle of the pole (0 = vertical), and the angular velocity. Let's render the environment... unfortunately we need to fix an annoying rendering issue first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K20AA1p5Vd7S"
      },
      "source": [
        "## Fixing the rendering issue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yDAdKKnZVd7T"
      },
      "source": [
        "Some environments (including the Cart-Pole) require access to your display, which opens up a separate window, even if you specify the `rgb_array` mode. In general you can safely ignore that window. However, if Jupyter is running on a headless server (ie. without a screen) it will raise an exception. One way to avoid this is to install a fake X server like Xvfb. You can start Jupyter using the `xvfb-run` command:\n",
        "\n",
        "    $ xvfb-run -s \"-screen 0 1400x900x24\" jupyter notebook\n",
        "\n",
        "If Jupyter is running on a headless server but you don't want to worry about Xvfb, then you can just use the following rendering function for the Cart-Pole:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CzsAGVPwVd7U",
        "colab": {}
      },
      "source": [
        "from PIL import Image, ImageDraw\n",
        "\n",
        "try:\n",
        "    from pyglet.gl import gl_info\n",
        "    openai_cart_pole_rendering = True   # no problem, let's use OpenAI gym's rendering function\n",
        "except Exception:\n",
        "    openai_cart_pole_rendering = False  # probably no X server available, let's use our own rendering function\n",
        "\n",
        "def render_cart_pole(env, obs):\n",
        "    if openai_cart_pole_rendering:\n",
        "        # use OpenAI gym's rendering function\n",
        "        return env.render(mode=\"rgb_array\")\n",
        "    else:\n",
        "        # rendering for the cart pole environment (in case OpenAI gym can't do it)\n",
        "        img_w = 600\n",
        "        img_h = 400\n",
        "        cart_w = img_w // 12\n",
        "        cart_h = img_h // 15\n",
        "        pole_len = img_h // 3.5\n",
        "        pole_w = img_w // 80 + 1\n",
        "        x_width = 2\n",
        "        max_ang = 0.2\n",
        "        bg_col = (255, 255, 255)\n",
        "        cart_col = 0x000000 # Blue Green Red\n",
        "        pole_col = 0x669acc # Blue Green Red\n",
        "\n",
        "        pos, vel, ang, ang_vel = obs\n",
        "        img = Image.new('RGB', (img_w, img_h), bg_col)\n",
        "        draw = ImageDraw.Draw(img)\n",
        "        cart_x = pos * img_w // x_width + img_w // x_width\n",
        "        cart_y = img_h * 95 // 100\n",
        "        top_pole_x = cart_x + pole_len * np.sin(ang)\n",
        "        top_pole_y = cart_y - cart_h // 2 - pole_len * np.cos(ang)\n",
        "        draw.line((0, cart_y, img_w, cart_y), fill=0)\n",
        "        draw.rectangle((cart_x - cart_w // 2, cart_y - cart_h // 2, cart_x + cart_w // 2, cart_y + cart_h // 2), fill=cart_col) # draw cart\n",
        "        draw.line((cart_x, cart_y - cart_h // 2, top_pole_x, top_pole_y), fill=pole_col, width=pole_w) # draw pole\n",
        "        return np.array(img)\n",
        "\n",
        "def plot_cart_pole(env, obs):\n",
        "    plt.close()  # or else nbagg sometimes plots in the previous cell\n",
        "    img = render_cart_pole(env, obs)\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QAGKqf-3Vd7X",
        "outputId": "63cb6d6a-222f-4ed9-f979-5186f302335f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        }
      },
      "source": [
        "plot_cart_pole(env, obs)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAABAZJREFUeJzt3EFqwlAUQNGmZEddU+maStfUNf0O\nHKpQMPp/uOeAE8HwBvHyCU+3McYbAA3vswcA4HVEHyBE9AFCRB8gRPQBQkQfIET0AUJEHyBE9AFC\nRB8gZJ89wB3+GwLg2vboBZz0AUJEHyBE9AFCRB8gRPQBQkQfIET0AUJEHyBE9AFCRB8gRPQBQkQf\nIET0AUJEHyBE9AFCRB8gRPQBQkQfIET0AUJEHyBE9AFCRB8gRPQBQkQfIET0AUJEHyBE9AFCRB8g\nRPQBQkQfIET0AUJEHyBE9AFCRB8gRPQBQkQfIET0AUJEHyBE9AFCRB8gRPQBQkQfIET0AUJEHyBE\n9AFCRB8gRPQBQkQfIET0AUJEHyBE9AFCRB8gRPQBQkQfIET0AUJEHyBE9AFCRB8gRPQBQkQfIET0\nAUJEHyBE9AFCRB8gRPQBQkQfIET0AUJEHyBE9AFCRB8gRPQBQkQfIET0AUJEHyBE9AFCRB8gRPQB\nQkQfIET0AUJEHyBE9AFCRB8gRPQBQkQfIET0AUJEHyBE9AFCRB8gRPQBQkQfIET0AUJEHyBE9AFC\nRB8gRPQBQkQfIET0AUJEHyBE9AFCRB8gRPQBQkQfIET0AUJEHyBE9AFCRB8gRPQBQkQfIET0AUJE\nHyBE9AFCRB8gRPQBQkQfIET0AUJEHyBE9AFCRB8gRPQBQkQfIET0AUJEHyBE9AFCRB8gRPQBQkQf\nIET0AUJEHyBE9AFCRB8gRPQBQkQfIET0AUJEHyBE9AFCRB8gRPQBQvbZA8Bqfn++rt77+PyeMAkc\nz0kfIET0AUJEHyBE9AFCRB8gRPQBQkQfIET0AUJEHyBE9AFCRB8gRPQBQkQfIET0AUJEHyBE9AFC\nRB8gRPQBQkQfIET0AUJEHyBE9AFCRB8gRPQBQkQfIET0AUJEHyBE9AFCRB8gRPQBQkQfIET0AUJE\nHyBE9AFCRB8gRPQBQkQfIET0AUJEHyBE9AFCRB8gRPQBQkQfIET0AUJEHyBE9AFCRB8gRPQBQkQf\nIET0AUJEHyBE9AFCRB8gRPQBQkQfIET0AUJEHyBE9AFCRB8gRPQBQkQfIET0Sdi27d+vRz9/7xqw\ngm2MMXuGW5YcivN6dYgX/V5xfg/fyE76ACGiDxAi+gAhog8QIvoAIfvsAW6x8sbZuYd5hiO2wpaM\nvnU3jmZlEy483gEIEX2AENEHCBF9gBDRBwgRfYCQJVc24WhWKOHCSR8gRPQBQlZ9vOM37ABP4KQP\nECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8Q\nIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACF/l5gcSMOFi5EAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "asaiqSllVd7b"
      },
      "source": [
        "Now let's look at the action space:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BQPM3m7ITYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Aq1roKfJVd7d",
        "outputId": "8476f0bc-3576-463b-8916-19420ebbd7d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "env.action_space"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cCbEvdSHVd7l"
      },
      "source": [
        "Yep, just two possible actions: accelerate towards the left or towards the right. Let's push the cart left until the pole falls:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IdoMByqbVd7m",
        "colab": {}
      },
      "source": [
        "obs = env.reset()\n",
        "while True:\n",
        "    obs, reward, done, info = env.step(0)\n",
        "    if done:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A0Y3k1nnVd7p",
        "outputId": "a974f67b-e73d-40fe-e1c5-336cb9ad60cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "plt.close()  # or else nbagg sometimes plots in the previous cell\n",
        "img = render_cart_pole(env, obs)\n",
        "plt.imshow(img)\n",
        "plt.axis(\"off\")\n",
        "#save_fig(\"cart_pole_plot\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.5, 599.5, 399.5, -0.5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAABGJJREFUeJzt3EtOAkEUQFHbsCPXpK5J3ZKuqR1g\n4gf8pBW6rXvOvKEGcFOpVzDN83wBQMPl2gsA4HxEHyBE9AFCRB8gRPQBQkQfIET0AUJEHyBE9AFC\nRB8gZLf2Aj7hvyEADk2/fQE7fYAQ0QcIEX2AENEHCBF9gBDRBwgRfYAQ0QcIEX2AENEHCBF9gBDR\nBwgRfYAQ0QcIEX2AENEHCBF9gBDRBwgRfYAQ0QcIEX2AENEHCBF9gBDRBwgRfYAQ0QcIEX2AENEH\nCBF9gBDRBwgRfYAQ0QcIEX2AENEHCBF9gBDRBwgRfYAQ0QcIEX2AENEHCBF9gBDRBwgRfYAQ0QcI\nEX2AENEHCBF9gBDRBwgRfYAQ0QcIEX2AENEHCBF9gBDRBwgRfYAQ0QcIEX2AENEHCBF9gBDRBwgR\nfYAQ0QcIEX2AENEHCBF9gBDRBwgRfYAQ0QcIEX2AENEHCBF9gBDRBwgRfYAQ0QcIEX2AENEHCBF9\ngBDRBwgRfYAQ0QcIEX2AENEHCBF9gBDRBwgRfYAQ0QcIEX2AENEHCBF9gBDRBwgRfYAQ0QcIEX2A\nENEHCBF9gBDRBwgRfYAQ0QcIEX2AENEHCBF9gBDRBwgRfYAQ0QcIEX2AENEHCBF9gBDRBwgRfYAQ\n0QcIEX2AENEHCBF9gBDRBwgRfYAQ0QcIEX2AENEHCBF9gBDRBwgRfYAQ0QcIEX2AENEHCBF9gBDR\nBwgRfYAQ0QcIEX2AENEHCBF9gBDRBwgRfYAQ0QcIEX2AENEHCBF9gBDRJ+vp4XbtJcDZiT5pwk/N\nbu0FwNrehv/q+m7FlcDp2enDC8GnQPRJcqxDlegDhIg+QIjoA4SIPlwY4tIh+uQY4lIm+uTZ5VMi\n+gAhog8QIvoAIaJPiiEudaJPmiEuNaIPECL6ZDjaAdEHSBF9gBDRJ8sQlyLRBwgRfRIMcWFP9AFC\nRB8gRPRJMsSlSvQBQkSf4RniwivRBwgRfYAQ0SfHEJcy0WdozvPhPdEHCBF9gBDRBwgRfVIMcakT\nfYZliAuHRJ8Mu3wQfYAU0QcIEX2AENFnSIa4cJzok2CIC3uiz3Ds8uFzog8QIvoAIaIPECL6DM8Q\nF16JPkMxxIWviT5AiOgDhIg+QIjoMzRDXHhP9BmGIS58T/QBQkQfIET0AUJEn2EZ4sIh0WcIhrjw\nM6IPECL6ACGiz5Cc58Nxu7UXAMdM07Toucf7m0XPz/O86P3gv5k2+mHf5KI4n6XRX2qj3wP46Ndf\nDMc7ACGiDxAi+gAhog8QIvoAIZu8snnumxvgM8d/8Be3zDYZfdfncGUTTsPxDkCI6AOEiD5AiOgD\nhIg+QIjoA4Rs8somuEIJp2GnDxAi+gAhWz3e8Zt4gBOw0wcIEX2AENEHCBF9gBDRBwgRfYAQ0QcI\nEX2AENEHCBF9gBDRBwgRfYAQ0QcIEX2AENEHCBF9gBDRBwgRfYAQ0QcIEX2AENEHCBF9gBDRBwgR\nfYAQ0QcIEX2AkGdnLUyjrgFI3gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NRM-Lf55Vd7t",
        "outputId": "b2791c50-3779-4882-9832-d51ee2d13f9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "img.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400, 600, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDZ97ME1ITY1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "h42DgS-qVd7z"
      },
      "source": [
        "Notice that the game is over when the pole tilts too much, not when it actually falls. Now let's reset the environment and push the cart to right instead:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VinwmA-qVd70",
        "colab": {}
      },
      "source": [
        "obs = env.reset()\n",
        "while True:\n",
        "    obs, reward, done, info = env.step(1)\n",
        "    if done:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IKmd8Gz8Vd73",
        "outputId": "891cd8ff-a6d8-4c19-c99b-81aa753752f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        }
      },
      "source": [
        "plot_cart_pole(env, obs)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAABJZJREFUeJzt3EFy00AQQFFE5UZwJcKZCFeCM4kd\nGxzKYEnTrv/eKouUPAvr11TPJNu+7x8AaPi4egEAXEf0AUJEHyBE9AFCRB8gRPQBQkQfIET0AUJE\nHyBE9AFCXlYv4B3+NwTAn7ZHH2CnDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIP\nECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8Q\nIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi\n+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6\nACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoA\nIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAh\nog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGi\nDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIP\nECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDwf6+f3r6iXA\nX237vq9ewy0jFwW33Ar9py/fFqyEgO3RB9jpA4SIPkCI6MODjHJ4JqIPJ3Cgy1SiDxAi+gAhog8Q\nIvpwAIe5PAvRh5M4zGUi0QcIEX2AENGHg5jr8wxEH05krs80og8QIvoAIaIPBzLXZzrRh5OZ6zOJ\n6MPB7PaZTPQBQkQfIET04QLm+kwh+nACc32mEn24iN0+E4g+QIjoA4SIPpzEXJ+JRB8uZK7PaqIP\nECL6ACGiDxAi+nAih7lMI/pwMYe5rCT6ACGiDxAi+nCyW3N9Ix5WEX2AENEHCBF9gBDRhwu4r88U\nog+LOMxlBdGHi9jtM4HoA4SIPkCI6MNC5vpcTfThQub6rCb6sJjdPlcSfYAQ0QcIEX24mLk+K4k+\nDGCuz1VEHyBE9AFCRB8WMNdnFdEHCBF9GMJhLlcQfYAQ0QcIEX1YxGEuK4g+DGKuz9m2fd9Xr+GW\nkYuCe2zbdvfv/nh7/f3z59e3//q8oe8w57j/y/XeA4Z+YUYuCu7xL9E/wtB3mHM8/OUy3gEIEX2A\nENEHCBF9gBDRBwh5Wb2AW66+/QDPzPvSccRNrZHRdwWNZ+bKJpMZ7wCEiD5AiOgDhIg+QIjoA4SI\nPkDIyCub8MxcoWQyO32AENEHCJk63vF35QAnsNMHCBF9gBDRBwgRfYAQ0QcIEX2AENEHCBF9gBDR\nBwgRfYAQ0QcIEX2AENEHCBF9gBDRBwgRfYAQ0QcIEX2AENEHCBF9gBDRBwgRfYAQ0QcIEX2AENEH\nCBF9gJBfOW5YyBCqX2QAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yWIKdTNITZF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DEZ_AtMNVd7-"
      },
      "source": [
        "Looks like it's doing what we're telling it to do. Now how can we make the poll remain upright? We will need to define a _policy_ for that. This is the strategy that the agent will use to select an action at each step. It can use all the past actions and observations to decide what to do."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8fN8zUKzVd7_"
      },
      "source": [
        "# A simple hard-coded policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sE_Wu9Z5Vd8B"
      },
      "source": [
        "Let's hard code a simple strategy: if the pole is tilting to the left, then push the cart to the left, and _vice versa_. Let's see if that works:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LqCJW559Vd8I",
        "colab": {}
      },
      "source": [
        "frames = []\n",
        "\n",
        "n_max_steps = 1000\n",
        "n_change_steps = 10\n",
        "\n",
        "obs = env.reset()\n",
        "for step in range(n_max_steps):\n",
        "    img = render_cart_pole(env, obs)\n",
        "    frames.append(img)\n",
        "\n",
        "    # hard-coded policy\n",
        "    position, velocity, angle, angular_velocity = obs\n",
        "    if angle < 0:\n",
        "        action = 0\n",
        "    else:\n",
        "        action = 1\n",
        "\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    if done:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bAWEdNRXVd8O",
        "outputId": "34e361f3-60a9-4eb0-badf-c372846f6e44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        }
      },
      "source": [
        "video = plot_animation(frames)\n",
        "plt.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAABAdJREFUeJzt3FFKw0AUQNGMZEeuSVyTuCbXNC6g\nLajVZOw9B/pTaHgf6WUIrx1zzg2AhqezBwDgOKIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGi\nDxCynz3ADf4bAuDSuPcCTvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8Q\nIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi\n+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6\nACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoA\nIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAh\nog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGi\nDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIP\nECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8Q\nIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAh+9kDwH/z8f568d7zy9sJk8D3\nOekDhIg+QIjoA4SIPkCI6AOEiD5AiOgDhIg+QIjoA4SIPkCI6AOEiD5AiOgDhIg+QIjoA4SIPkCI\n6AOEiD5AiOgDhIg+QIjoA4SIPkCI6AOEiD5AiOgDhIg+QIjoA4SIPkCI6AOEiD5AiOgDhIg+QIjo\nA4SIPkCI6AOEiD5AiOgDhIg+QIjoA4SIPkCI6AOEiD5AiOgDhIg+QIjoA4SIPkCI6AOEiD5AiOgD\nhIg+QIjoA4SIPkCI6AOEiD5AiOgDhIg+QIjoA4SIPkCI6AOEiD5AiOgDhIg+bNs2xvjy697P37oG\nHGHMOc+e4Zolh+JxHR3iRb93rO/uG9VJHyBE9AFCRB8gRPQBQkQfIGQ/e4BrrLTx6Nzj/MRvbH0t\nGX3rbBzNyiYVHu8AhIg+QIjoA4SIPkCI6AOEiD5AyJIrm3A0K5RUOOkDhIg+QMiqj3f8Rh3gDzjp\nA4SIPkCI6AOEiD5AiOgDhIg+QIjoA4SIPkCI6AOEiD5AiOgDhIg+QIjoA4SIPkCI6AOEiD5AiOgD\nhIg+QIjoA4SIPkCI6AOEiD5AiOgDhIg+QIjoA4SIPkDIJw7pHEiXCnWwAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_C9N67QITZb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gVz8nG0yVd8W"
      },
      "source": [
        "Nope, the system is unstable and after just a few wobbles, the pole ends up too tilted: game over. We will need to be smarter than that!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "deLg1rpdVd8X"
      },
      "source": [
        "# Neural Network Policies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Su9ePJSwVd8Y"
      },
      "source": [
        "Let's create a neural network that will take observations as inputs, and output the action to take for each observation. To choose an action, the network will first estimate a probability for each action, then select an action randomly according to the estimated probabilities. In the case of the Cart-Pole environment, there are just two possible actions (left or right), so we only need one output neuron: it will output the probability `p` of the action 0 (left), and of course the probability of action 1 (right) will be `1 - p`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "do0awIeVVd8b"
      },
      "source": [
        "Note: instead of using the `fully_connected()` function from the `tensorflow.contrib.layers` module (as in the book), we now use the `dense()` function from the `tf.layers` module, which did not exist when this chapter was written. This is preferable because anything in contrib may change or be deleted without notice, while `tf.layers` is part of the official API. As you will see, the code is mostly the same.\n",
        "\n",
        "The main differences relevant to this chapter are:\n",
        "* the `_fn` suffix was removed in all the parameters that had it (for example the `activation_fn` parameter was renamed to `activation`).\n",
        "* the `weights` parameter was renamed to `kernel`,\n",
        "* the default activation is `None` instead of `tf.nn.relu`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BXRY_Q3fVd8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "5510e576-5380-4aa8-931a-50bded6b494d"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 1. Specify the network architecture\n",
        "n_inputs = 4  # == env.observation_space.shape[0]\n",
        "n_hidden = 4  # it's a simple task, we don't need more than this\n",
        "n_outputs = 1 # only outputs the probability of accelerating left\n",
        "initializer = tf.variance_scaling_initializer()\n",
        "\n",
        "# 2. Build the neural network\n",
        "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
        "hidden = tf.layers.dense(X, n_hidden, activation=tf.nn.elu,\n",
        "                         kernel_initializer=initializer)\n",
        "outputs = tf.layers.dense(hidden, n_outputs, activation=tf.nn.sigmoid,\n",
        "                          kernel_initializer=initializer)\n",
        "\n",
        "# 3. Select a random action based on the estimated probabilities\n",
        "p_left_and_right = tf.concat(axis=1, values=[outputs, 1 - outputs])\n",
        "#CellStrat - tf.multinomial picks a random value from the two values\n",
        "action = tf.multinomial(tf.log(p_left_and_right), num_samples=1)\n",
        "\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0824 05:51:00.842581 140204133070720 deprecation.py:323] From <ipython-input-24-e6cd743d41f3>:12: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "W0824 05:51:01.118531 140204133070720 deprecation.py:323] From <ipython-input-24-e6cd743d41f3>:19: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u3vN_5eDVd8f"
      },
      "source": [
        "In this particular environment, the past actions and observations can safely be ignored, since each observation contains the environment's full state. If there were some hidden state then you may need to consider past actions and observations in order to try to infer the hidden state of the environment. For example, if the environment only revealed the position of the cart but not its velocity, you would have to consider not only the current observation but also the previous observation in order to estimate the current velocity. Another example is if the observations are noisy: you may want to use the past few observations to estimate the most likely current state. Our problem is thus as simple as can be: the current observation is noise-free and contains the environment's full state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aESdtrFGVd8f"
      },
      "source": [
        "You may wonder why we are picking a random action based on the probability given by the policy network, rather than just picking the action with the highest probability. This approach lets the agent find the right balance between _exploring_ new actions and _exploiting_ the actions that are known to work well. Here's an analogy: suppose you go to a restaurant for the first time, and all the dishes look equally appealing so you randomly pick one. If it turns out to be good, you can increase the probability to order it next time, but you shouldn't increase that probability to 100%, or else you will never try out the other dishes, some of which may be even better than the one you tried."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0s5ejjMJVd8g"
      },
      "source": [
        "Let's randomly initialize this policy neural network and use it to play one game:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7xa8IJNVVd8i",
        "colab": {}
      },
      "source": [
        "n_max_steps = 1000\n",
        "frames = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    obs = env.reset()\n",
        "    for step in range(n_max_steps):\n",
        "        img = render_cart_pole(env, obs)\n",
        "        frames.append(img)\n",
        "        action_val = action.eval(feed_dict={X: obs.reshape(1, n_inputs)})\n",
        "        obs, reward, done, info = env.step(action_val[0][0])\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bbjAfADFVd8l"
      },
      "source": [
        "Now let's look at how well this randomly initialized policy network performed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1MmA9xLcVd8m",
        "outputId": "6117fe2d-474e-4454-82aa-58e20b90816d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        }
      },
      "source": [
        "video = plot_animation(frames)\n",
        "plt.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAABAtJREFUeJzt3EFqwlAUQNGmZEddU3FNxTW5pt9x\nqUIhiT/2ngMOHBjeIF4+4ekyxngDoOF99gAAPI/oA4SIPkCI6AOEiD5AiOgDhIg+QIjoA4SIPkCI\n6AOErLMHeMB/QwD8tmy9gJM+QIjoA4SIPkCI6AOEiD5AiOgDhIg+QIjoA4SIPkCI6AOEiD5AiOgD\nhIg+QIjoA4SIPkCI6AOEiD5AiOgDhIg+QIjoA4SIPkCI6AOEiD5AiOgDhIg+QIjoA4SIPkCI6AOE\niD5AiOgDhIg+QIjoA4SIPkCI6AOEiD5AiOgDhIg+QIjoA4SIPkCI6AOEiD5AiOgDhIg+QIjoA4SI\nPkCI6AOEiD5AiOgDhIg+QIjoA4SIPkCI6AOEiD5AiOgDhIg+QIjoA4SIPkCI6AOEiD5AiOgDhIg+\nQIjoA4SIPkCI6AOEiD5AiOgDhIg+QIjoA4SIPkCI6AOEiD5AiOgDhIg+QIjoA4SIPkCI6AOEiD5A\niOgDhIg+QIjoA4SIPkCI6AOEiD5AiOgDhIg+QIjoA4SIPkCI6AOEiD5AiOgDhIg+QIjoA4SIPkCI\n6AOEiD5AiOgDhIg+QIjoA4SIPkCI6AOEiD5AiOgDhIg+QIjoA4SIPkCI6AOEiD5AiOgDhIg+QIjo\nA4SIPkCI6AOEiD5AiOgDhIg+QIjoA4SIPkCI6AOEiD5AiOgDhIg+QIjoA4SIPkCI6AOEiD5AiOgD\nhIg+QIjoA4SIPkCI6AOEiD5AiOgDhIg+QIjoA4SIPkCI6AOEiD5AyDp7ADir2/Xy4/3H59ekSWA/\nTvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi\n+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvpwx+16mT0C\nHEL0AUJEHyBE9AFCRB8gRPQBQkQfIET0AUJEHyBE9AFCRB8gRPQBQkQfIET0AUJEHyBE9AFCRB8g\nRPQBQkQfIET0AUJEHyBE9AFCRB8gRPQBQkQfIET0AUJEHyBE9AFCRB8gRPQBQkQfIET0yViW5c+v\nrZ9/dA2YbRljzJ7hnlMOxWt7dohP+t3itW2+iZ30AUJEHyBE9AFCRB8gRPQBQtbZA9xj3Y3/wH3M\n3vbYCDtl9K26cQQrm+DxDkCK6AOEiD5AiOgDhIg+QIjoA4SccmUTjmCFEpz0AVJEHyDkrI93/H4d\n4ABO+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIP\nECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8Q8g3DTx4Xy1shXAAAAABJRU5ErkJg\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GkSq9wS9Vd8p"
      },
      "source": [
        "Yeah... pretty bad. The neural network will have to learn to do better. First let's see if it is capable of learning the basic policy we used earlier: go left if the pole is tilting left, and go right if it is tilting right. The following code defines the same neural network but we add the target probabilities `y`, and the training operations (`cross_entropy`,  `optimizer` and `training_op`):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "os3PMnWPVd8q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "outputId": "240104c2-2043-42ad-eb2f-ea393c1f9296"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "reset_graph()\n",
        "\n",
        "n_inputs = 4\n",
        "n_hidden = 4\n",
        "n_outputs = 1\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "initializer = tf.variance_scaling_initializer()\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
        "y = tf.placeholder(tf.float32, shape=[None, n_outputs])\n",
        "\n",
        "hidden = tf.layers.dense(X, n_hidden, activation=tf.nn.elu, kernel_initializer=initializer)\n",
        "logits = tf.layers.dense(hidden, n_outputs)\n",
        "outputs = tf.nn.sigmoid(logits) # probability of action 0 (left)\n",
        "p_left_and_right = tf.concat(axis=1, values=[outputs, 1 - outputs])\n",
        "action = tf.multinomial(tf.log(p_left_and_right), num_samples=1)\n",
        "\n",
        "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "training_op = optimizer.minimize(cross_entropy)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0824 05:52:59.558245 140204133070720 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0824 05:52:59.581680 140204133070720 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fX4S6fCfVd8s"
      },
      "source": [
        "We can make the same net play in 10 different environments in parallel, and train for 1000 iterations. We also reset environments when they are done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ntx66ODeVd8u",
        "colab": {}
      },
      "source": [
        "n_environments = 10\n",
        "n_iterations = 1000\n",
        "\n",
        "envs = [gym.make(\"CartPole-v0\") for _ in range(n_environments)]\n",
        "observations = [env.reset() for env in envs]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for iteration in range(n_iterations):\n",
        "        target_probas = np.array([([1.] if obs[2] < 0 else [0.]) for obs in observations]) # if angle<0 we want proba(left)=1., or else proba(left)=0.\n",
        "        action_val, _ = sess.run([action, training_op], feed_dict={X: np.array(observations), y: target_probas})\n",
        "        for env_index, env in enumerate(envs):\n",
        "            obs, reward, done, info = env.step(action_val[env_index][0])\n",
        "            observations[env_index] = obs if not done else env.reset()\n",
        "    saver.save(sess, \"./my_policy_net_basic.ckpt\")\n",
        "\n",
        "for env in envs:\n",
        "    env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UxZQkIU0Vd82",
        "colab": {}
      },
      "source": [
        "def render_policy_net(model_path, action, X, n_max_steps = 1000):\n",
        "    frames = []\n",
        "    env = gym.make(\"CartPole-v0\")\n",
        "    obs = env.reset()\n",
        "    with tf.Session() as sess:\n",
        "        saver.restore(sess, model_path)\n",
        "        for step in range(n_max_steps):\n",
        "            img = render_cart_pole(env, obs)\n",
        "            frames.append(img)\n",
        "            action_val = action.eval(feed_dict={X: obs.reshape(1, n_inputs)})\n",
        "            obs, reward, done, info = env.step(action_val[0][0])\n",
        "            if done:\n",
        "                break\n",
        "    env.close()\n",
        "    return frames        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gI-0C0lcVd84",
        "outputId": "cc8add29-2f5d-4730-c438-25adfe30280e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "source": [
        "frames = render_policy_net(\"./my_policy_net_basic.ckpt\", action, X)\n",
        "video = plot_animation(frames)\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0824 05:53:49.183876 140204133070720 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAABAdJREFUeJzt3EFKw1AUQFEj2ZFrEtckrsk1fceS\nCELa5Lf3HOigg5Q3SC+f8NpljPECQMPr1QMAcB7RBwgRfYAQ0QcIEX2AENEHCBF9gBDRBwgRfYAQ\n0QcIWa8e4A/+GwJgazn6AU76ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIP\nECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8Q\nIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi\n+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6\nACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoA\nIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAh\nog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGi\nDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIP\nECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIevVA8AMvr8+fr1/e/+8aBK4\nLyd9gBDRBwgRfYAQ0QcIEX2AENEHCBF9gBDRBwgRfYAQ0QcIEX2AENEHCBF9gBDRBwgRfYAQ0QcI\nEX2AENEHCBF9gBDRBwgRfYAQ0QcIEX2AENEHCBF9gBDRBwgRfYAQ0QcIEX2AENEHCBF9gBDRBwgR\nfYAQ0QcIEX2AENEHCBF9gBDRBwgRfYAQ0QcIEX2AENEHCBF9gBDRBwgRfYAQ0QcIEX2AENEHCBF9\ngBDRBwgRfYAQ0QcIEX2AENEHCBF9gBDRBwgRfYAQ0QcIEX2AENEHCBF9gBDR52kty/Lv15Fr966H\nWS1jjKtn2DPlUDyWM2M86feI53P4pnbSBwgRfYAQ0QcIEX2AENEHCFmvHmCPFTgejXuWM9xiS2zK\n6Ft/4xasbMKWxzsAIaIPECL6ACGiDxAi+gAhog8QMuXKJtyCNUrYctIHCBF9gJBZH+/4TTvAHTjp\nA4SIPkCI6AOEiD5AiOgDhIg+QIjoA4SIPkCI6AOEiD5AiOgDhIg+QIjoA4SIPkCI6AOEiD5AiOgD\nhIg+QIjoA4SIPkCI6AOEiD5AiOgDhIg+QIjoA4SIPkDID0grHEh1nOt5AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r4T2HdVWVd87"
      },
      "source": [
        "Looks like it learned the policy correctly. Now let's see if it can learn a better policy on its own."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wTM72EHyVd88"
      },
      "source": [
        "# Policy Gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "np12OnhwVd89"
      },
      "source": [
        "To train this neural network we will need to define the target probabilities `y`. If an action is good we should increase its probability, and conversely if it is bad we should reduce it. But how do we know whether an action is good or bad? The problem is that most actions have delayed effects, so when you win or lose points in a game, it is not clear which actions contributed to this result: was it just the last action? Or the last 10? Or just one action 50 steps earlier? This is called the _credit assignment problem_.\n",
        "\n",
        "The _Policy Gradients_ algorithm tackles this problem by first playing multiple games, then making the actions in good games slightly more likely, while actions in bad games are made slightly less likely. First we play, then we go back and think about what we did."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a4tOfKciVd89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "outputId": "56221dc6-5160-44f3-f82c-ce703ef8732d"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "reset_graph()\n",
        "\n",
        "n_inputs = 4\n",
        "n_hidden = 4\n",
        "n_outputs = 1\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "initializer = tf.variance_scaling_initializer()\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
        "\n",
        "hidden = tf.layers.dense(X, n_hidden, activation=tf.nn.elu, kernel_initializer=initializer)\n",
        "logits = tf.layers.dense(hidden, n_outputs)\n",
        "outputs = tf.nn.sigmoid(logits)  # probability of action 0 (left)\n",
        "p_left_and_right = tf.concat(axis=1, values=[outputs, 1 - outputs])\n",
        "action = tf.multinomial(tf.log(p_left_and_right), num_samples=1)\n",
        "\n",
        "y = 1. - tf.to_float(action)\n",
        "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "grads_and_vars = optimizer.compute_gradients(cross_entropy)\n",
        "gradients = [grad for grad, variable in grads_and_vars]\n",
        "gradient_placeholders = []\n",
        "grads_and_vars_feed = []\n",
        "for grad, variable in grads_and_vars:\n",
        "    gradient_placeholder = tf.placeholder(tf.float32, shape=grad.get_shape())\n",
        "    gradient_placeholders.append(gradient_placeholder)\n",
        "    grads_and_vars_feed.append((gradient_placeholder, variable))\n",
        "training_op = optimizer.apply_gradients(grads_and_vars_feed)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0824 05:54:00.506054 140204133070720 deprecation.py:323] From <ipython-input-31-d42d007ce403>:21: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5dhMu61ZVd8_",
        "colab": {}
      },
      "source": [
        "def discount_rewards(rewards, discount_rate):\n",
        "    discounted_rewards = np.zeros(len(rewards))\n",
        "    cumulative_rewards = 0\n",
        "    for step in reversed(range(len(rewards))):\n",
        "        cumulative_rewards = rewards[step] + cumulative_rewards * discount_rate\n",
        "        discounted_rewards[step] = cumulative_rewards\n",
        "    return discounted_rewards\n",
        "\n",
        "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
        "    all_discounted_rewards = [discount_rewards(rewards, discount_rate) for rewards in all_rewards]\n",
        "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
        "    reward_mean = flat_rewards.mean()\n",
        "    reward_std = flat_rewards.std()\n",
        "    return [(discounted_rewards - reward_mean)/reward_std for discounted_rewards in all_discounted_rewards]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JkhOXx5NVd9B",
        "outputId": "e5dcb684-0a87-4d79-a984-26d249b4bc75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "discount_rewards([10, 0, -50], discount_rate=0.8)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-22., -40., -50.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4pHYpKNhVd9D",
        "outputId": "5a12ec40-8891-4962-ac6c-5eeaeda010b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "discount_and_normalize_rewards([[10, 0, -50], [10, 20]], discount_rate=0.8)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([-0.28435071, -0.86597718, -1.18910299]),\n",
              " array([1.26665318, 1.0727777 ])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IVcBTcesVd9F",
        "outputId": "a80a994b-5233-414e-9fcb-516d5dc96fab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "n_games_per_update = 10\n",
        "n_max_steps = 1000\n",
        "n_iterations = 250\n",
        "save_iterations = 10\n",
        "discount_rate = 0.95\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for iteration in range(n_iterations):\n",
        "        print(\"\\rIteration: {}\".format(iteration), end=\"\")\n",
        "        all_rewards = []\n",
        "        all_gradients = []\n",
        "        for game in range(n_games_per_update):\n",
        "            current_rewards = []\n",
        "            current_gradients = []\n",
        "            obs = env.reset()\n",
        "            for step in range(n_max_steps):\n",
        "                action_val, gradients_val = sess.run([action, gradients], feed_dict={X: obs.reshape(1, n_inputs)})\n",
        "                obs, reward, done, info = env.step(action_val[0][0])\n",
        "                current_rewards.append(reward)\n",
        "                current_gradients.append(gradients_val)\n",
        "                if done:\n",
        "                    break\n",
        "            all_rewards.append(current_rewards)\n",
        "            all_gradients.append(current_gradients)\n",
        "\n",
        "        all_rewards = discount_and_normalize_rewards(all_rewards, discount_rate=discount_rate)\n",
        "        feed_dict = {}\n",
        "        for var_index, gradient_placeholder in enumerate(gradient_placeholders):\n",
        "            mean_gradients = np.mean([reward * all_gradients[game_index][step][var_index]\n",
        "                                      for game_index, rewards in enumerate(all_rewards)\n",
        "                                          for step, reward in enumerate(rewards)], axis=0)\n",
        "            feed_dict[gradient_placeholder] = mean_gradients\n",
        "        sess.run(training_op, feed_dict=feed_dict)\n",
        "        if iteration % save_iterations == 0:\n",
        "            saver.save(sess, \"./my_policy_net_pg.ckpt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 192"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qpdPMBqOVd9I",
        "colab": {}
      },
      "source": [
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "thfzQ4TgVd9K",
        "colab": {},
        "outputId": "35200074-cdfc-44b1-80b9-3a2630718b49"
      },
      "source": [
        "frames = render_policy_net(\"./my_policy_net_pg.ckpt\", action, X, n_max_steps=1000)\n",
        "video = plot_animation(frames)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./my_policy_net_pg.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAABDhJREFUeJzt3NtpAlEARdFMsInUYRupQ2vSOtKGdaSMyV8QH0RQc9W9FghGUM5H3FyGZKZ5nt8AaHgfPQCA/yP6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhi9EDznBvCIBj07Uf4KQPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxCyGD0AHsluuz56bbnaDFgC9+GkDxAi+gAhog9/OHXJB56V6AOEiD5AiOgDhIg+QIjoA4SIPkCI6AOEiD5AiOgDhIg+QIjowx531OTViT5AiOgDhIg+QIjoA4SIPkCI6AOEiD5AiOgDhIg+QIjoA4SIPlxgt12PngA3IfoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+nBgudqMngB3I/oAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+nCh3XY9egJcTfQBQkQfIET0AUJEHyBE9AFCRB8gRPQBQkQfIET0AUJEHyBE9AFCRB9OWK42oyfAXYg+QIjokzJN08WPe7wfRhN9gJDF6AHwyL6+V7/PPz+2A5fAbTjpwxn7wT/1Mzwj0QcIEX2AENGHMw6v4bumzyuY5nkeveGUhxzF8/vPP6V80O8Wz+3qX2AnfYAQ0QcIEX2AENEHCBF9gBDRBwgRfYAQ0QcIEX2AELdWJsV/yVLnpA8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPELIYPeCMafQAgFfkpA8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIaIPECL6ACGiDxAi+gAhog8QIvoAIT/o9SVDyNr7QQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}